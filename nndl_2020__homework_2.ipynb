{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nndl_2020__homework_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanieleFoscarin/NNDL_hw2/blob/main/nndl_2020__homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYncZVOyF8Rb"
      },
      "source": [
        "#NEURAL NETWORKS AND DEEP LEARNING\n",
        "> M.Sc. ICT FOR LIFE AND HEALTH\n",
        "> \n",
        "> Department of Information Engineering\n",
        "\n",
        "> M.Sc. COMPUTER ENGINEERING\n",
        ">\n",
        "> Department of Information Engineering\n",
        "\n",
        "> M.Sc. AUTOMATION ENGINEERING\n",
        ">\n",
        "> Department of Information Engineering\n",
        " \n",
        "> M.Sc. PHYSICS OF DATA\n",
        ">\n",
        "> Department of Physics and Astronomy\n",
        " \n",
        "> M.Sc. COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY\n",
        ">\n",
        "> Department of General Psychology\n",
        "\n",
        "---\n",
        "A.A. 2020/21 (6 CFU) - Dr. Alberto Testolin, Dr. Matteo Gadaleta\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHJFDwYvgHg"
      },
      "source": [
        "# Homework 2 - Unsupervised Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNENm7RHGFMd"
      },
      "source": [
        "## General overview\n",
        "In this homework you will learn how to implement and test neural network models for solving unsupervised problems. For simplicity and to allow continuity with the kind of data you have seen before, the homework will be based on images of handwritten digits (MNIST). However, you can optionally explore different image collections (e.g., [Caltech](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) or [Cifar](https://www.cs.toronto.edu/~kriz/cifar.html)) or other datasets based on your interests. The basic tasks for the homework will require to test and analyze the convolutional autoencoder implemented during the Lab practice. If you prefer, you can opt for a fully-connected autoencoder, which should achieve similar performance considering the relatively small size of the MNIST images. As for the previous homework, you should explore the use of advanced optimizers and regularization methods. Learning hyperparameters should be tuned using appropriate search procedures, and final accuracy should be evaluated using a cross-validation setup. More advanced tasks will require the exploration of denoising and variational architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZGjMokMvk2h"
      },
      "source": [
        "\n",
        "## Technical notes\n",
        "The homework should be implemented in Python using the PyTorch framework. The student can explore additional libraries and tools to implement the models; however, please make sure you understand the code you are writing because during the exam you might receive specific questions related to your implementation. The entire source code required to run the homework must be uploaded as a compressed archive in a Moodle section dedicated to the homework. If your code will be entirely included in a single Python notebook, just upload the notebook file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZUBEMCvlpB"
      },
      "source": [
        "\n",
        "## Final report\n",
        "Along with the source code, you must separately upload a PDF file containing a brief report of your homework. The report should include a brief Introduction on which you explain the homework goals and the main implementation strategies you choose, a brief Method section where you describe your model architectures and hyperparameters, and a Result section where you present the simulation results. Total length must not exceed 6 pages, though you can include additional tables and figures in a final Appendix (optional).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RchPc7G6vmRB"
      },
      "source": [
        "\n",
        "## Grade\n",
        "The maximum grade for this homework will be **8 points**. Points will be assigned based on the correct implementation of the following items:\n",
        "*\t1 pt: implement and test (convolutional) autoencoder, reporting the trend of reconstruction loss and some examples of image reconstruction\n",
        "*\t1 pt: explore advanced optimizers and regularization methods \n",
        "*\t1 pt: optimize hyperparameters using grid/random search and cross-validation\n",
        "*\t1 pt: implement and test denoising (convolutional) autoencoder\n",
        "*\t1 pt: fine-tune the (convolutional) autoencoder using a supervised classification task (you can compare classification accuracy and learning speed with results achieved in homework 1)\n",
        "*\t1 pt: explore the latent space structure (e.g., PCA, t-SNE) and generate new samples from latent codes\n",
        "*\t2 pt: implement variational (convolutional) autoencoder or GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYE6Cjhgvm3B"
      },
      "source": [
        "\n",
        "## Deadline\n",
        "The complete homework (source code + report) must be submitted through Moodle at least 10 days before the chosen exam date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCH18n_rC4X9"
      },
      "source": [
        "#################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSqUbbfJC3gG",
        "outputId": "404969e8-0417-42ed-8d51-45618bc03b4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt # plotting library\r\n",
        "import numpy as np # this module is useful to work with numerical arrays\r\n",
        "import pandas as pd # this module is useful to work with tabular data\r\n",
        "import random # this module will be used to select random samples from a collection\r\n",
        "import os # this module will be used just to create directories in the local filesystem\r\n",
        "from tqdm import tqdm # this module is useful to plot progress bars\r\n",
        "\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torch import nn\r\n",
        "from torchvision import transforms\r\n",
        "import torchvision\r\n",
        "\r\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
        "print(f\"Training device: {device}\")\r\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training device: cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe61a632048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvrUpH2TAXro"
      },
      "source": [
        "\r\n",
        "# Dataset\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxHnXmGwAzHZ"
      },
      "source": [
        "get notMNIST dataset from repo\r\n",
        "It is the small version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1CDXwDpA6rg",
        "outputId": "42d71d9b-b6d9-48bb-d84f-4b153d0a3d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/DanieleFoscarin/NNDL_hw2.git"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NNDL_hw2'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 37 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6knS0bSBdMu"
      },
      "source": [
        "! tar xzf  /content/NNDL_hw2/notMNIST_small.tar.gz"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1-8FGfPDdg9",
        "outputId": "0840783e-cc6e-45a6-927f-879eb0dd3e7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ROOT = '/content/notMNIST_small' \r\n",
        "from pathlib import Path\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "images = []\r\n",
        "labels = []\r\n",
        "\r\n",
        "#there are few broken files that cannot be read\r\n",
        "#not a big problem since they are 5 over 500k\r\n",
        "num_label = 0\r\n",
        "for folder in sorted(Path(ROOT).iterdir()):\r\n",
        "  print(folder)\r\n",
        "  for f in folder.iterdir():\r\n",
        "    try:     \r\n",
        "      img = Image.open(f)\r\n",
        "      img_array = np.asarray(img)\r\n",
        "      images.append(img_array)\r\n",
        "      labels.append(num_label)\r\n",
        "    except:\r\n",
        "      print(\"a file was not recognized\")\r\n",
        "  num_label+=1\r\n",
        "\r\n",
        "\r\n",
        "images_arr = np.float32(np.array(images))\r\n",
        "labels_arr = np.float32(np.array(labels))\r\n",
        "\r\n",
        "#free up some memory\r\n",
        "del images\r\n",
        "\r\n",
        "# #split dataset for classification purposes if needed\r\n",
        "# X_train, X_test, y_train, y_test = train_test_split(images_arr, labels_arr, test_size=0.10, random_state=42)\r\n",
        "\r\n",
        "# print(np.shape(X_train),' ', np.shape(y_train),' ', np.shape(X_test),' ', np.shape(y_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/notMNIST_small/A\n",
            "a file was not recognized\n",
            "/content/notMNIST_small/B\n",
            "/content/notMNIST_small/C\n",
            "/content/notMNIST_small/D\n",
            "/content/notMNIST_small/E\n",
            "/content/notMNIST_small/F\n",
            "a file was not recognized\n",
            "/content/notMNIST_small/G\n",
            "/content/notMNIST_small/H\n",
            "/content/notMNIST_small/I\n",
            "/content/notMNIST_small/J\n",
            "(16851, 28, 28)   (16851,)   (1873, 28, 28)   (1873,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSRwqa25PdT7",
        "outputId": "941fda1c-ff5a-4ef5-9b5d-6fa867015647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(labels_arr)\r\n",
        "print(np.shape(labels_arr))\r\n",
        "print(np.shape(images_arr))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 9. 9. 9.]\n",
            "(18724,)\n",
            "(18724, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-XaJMG6NCDB",
        "outputId": "95d4de7e-2607-477a-9d98-2347ee8fa34d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#not sure the transformation is done correctly\r\n",
        "class NMDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, image, label, transform=None):\r\n",
        "        self.transform = transform\r\n",
        "        self.image = image\r\n",
        "        self.label = label\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        #return a nparray from the dataframe\r\n",
        "        this_image = self.image[idx]\r\n",
        "        this_label = self.label[idx]\r\n",
        "        sample = [this_image, this_label]\r\n",
        "        if self.transform:\r\n",
        "            sample[0] = self.transform(sample[0])\r\n",
        "        return sample\r\n",
        "\r\n",
        "\r\n",
        "    class ToTensor(object):\r\n",
        "        \"\"\"Convert sample to Tensors.\"\"\"\r\n",
        "\r\n",
        "        def __call__(self, sample):\r\n",
        "            x, y = sample\r\n",
        "            return (torch.tensor([x]).float(),\r\n",
        "                    torch.tensor([y]).float())\r\n",
        "\r\n",
        "to_tensor = torchvision.transforms.ToTensor()   \r\n",
        "# composed_transform = transforms.Compose([ToTensor()])\r\n",
        "dataset = NMDataset(images_arr, labels_arr, transform=to_tensor)\r\n",
        "\r\n",
        "#check correctness\r\n",
        "out = dataset.__getitem__(0)\r\n",
        "print(labels_arr[0])\r\n",
        "print(out[1])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygKFewCWX7eS"
      },
      "source": [
        "Split dataset in train, validation, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AiWkPiRYCsT",
        "outputId": "7e9b321a-28e5-432f-bab1-da850bf1f355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "length = dataset.__len__()\r\n",
        "testval_size = int(np.ceil(length*0.4))\r\n",
        "train_size = int(length-testval_size)\r\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, testval_size])\r\n",
        "\r\n",
        "test_size = int(np.ceil(test_dataset.__len__()*0.5))\r\n",
        "val_size = int(test_dataset.__len__()-test_size)\r\n",
        "test_dataset, val_dataset = torch.utils.data.random_split(test_dataset, [test_size, val_size])\r\n",
        "\r\n",
        "print(f\"train_dataset length {train_dataset.__len__()}\")\r\n",
        "print(f\"val_dataset length {val_dataset.__len__()}\")\r\n",
        "print(f\"test_dataset length {test_dataset.__len__()}\")\r\n",
        "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7490 11234\n",
            "3745 3745\n",
            "train_dataset length 11234\n",
            "val_dataset length 3745\n",
            "test_dataset length 3745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2sW0A9WX67b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDeLbLmbQ6zX"
      },
      "source": [
        "#just to check database functions\r\n",
        "to_tensor = torchvision.transforms.ToTensor()\r\n",
        "test_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True, transform=to_tensor)"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}